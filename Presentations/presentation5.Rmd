---
title: "V.\ Statistical analysis in R (presentation)"
author: "Science Data Lab & Center for Health Data Science"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  #pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, warning = FALSE)
```

## Structure of a Biostatistical Analysis in R

The very basic structure of an R script doing a classical statistical analysis is as follows:

*  Load packages that you will be using.   
*  Read the dataset to be analyzed. Possibly also do some data cleaning and manipulation.   
*  Visualize the dataset by graphics and other descriptive statistics.   
*  Fit and validate a statistical model.   
*  Hypothesis testing. Possibly also post hoc testing.   
*  Report model parameters and/or model predictions.   
*  Visualize your results.   

Of course there are variants of this set-up, and in practice there will often be some iterations of the steps.   
In this manuscript we will exemplify the proposed steps in the analysis of two simple datasets:   

*    One-way ANOVA of the expression of the IGFL4 gene against the skin type in psoriasis patients.
*    Multiple linear regression of volume of cherry trees against their diameter and height.




####  Load packages

For both examples we will use **ggplot2** to make plots, and to be prepared for data manipulations, we simply load this together with the rest of the **tidyverse**. Moreover, we will also use extra plotting functionalities from the **GGally** package.

The psoriasis data are provided in an Excel sheet, so we also load **readxl**. Finally, we will use the package **emmeans** to make post hoc tests and to report model predictions.

Remember that you should install the wanted packages before they can be used (but you only need to install the packages once!). 

Thus,
```{r warning=FALSE, message=FALSE}
#install.packages("tidyverse")
#install.packages("GGally")
#install.packages("readxl")
#install.packages("emmeans")
library(tidyverse)
library(GGally)
library(readxl)
library(emmeans)
```

Now we have done step 1 for both analyses. Next we will look specifically at the two examples. Finally, we conclude with a brief outlook on other statistical models in R.


## Example A: Analysis of variance

#### Step 1: Data

Psoriasis is an immune-mediated disease that affects the skin. Researchers carried out a micro-array experiment with skin from 37 people in order to examine a potential association between the disease and a certain gene (IGFL4). For each of the 37 samples the gene expression was measured. Fifteen skin samples were from psoriasis patients and from a part of the body affected by the disease (`psor`); 15 samples were from psoriasis patients but from a part of the body not affected by the disease (`psne`); and 7 skin samples were from healthy people (`control`).  

The data are saved in the file **psoriasis.xlsx**. At first the variable `type` is stored as a character variable, we change it to a factor (and check that indeed there are 15, 15 and 7 patients in the three groups).

```{r}
psoriasisData <- read_excel("psoriasis.xlsx")

# Dataset1
psorData  <- select(psoriasisData, type, IGFL4)

# Dataset2
psorDataG <- select(psoriasisData, -IGFL4)


psorData <- mutate(psorData, type = factor(type))
count(psorData, type)
```


#### Step 2: Descriptive plots and statistics

To get an impression of the data we make two plots and compute group-wise means and standard deviations.

```{r, fig.width=3.5, fig.asp=0.618, fig.align = "default", fig.show = "hold"}
ggplot(psorData, aes(x=type, y=IGFL4)) +
  geom_point() + 
  labs(x="Skin type", y="IGFL4")

ggplot(psorData, aes(x=type, y=IGFL4)) +
  geom_boxplot() + 
  labs(x="Skin type", y="IGFL4")

psorData %>% 
  group_by(type) %>% 
  summarise(avg=mean(IGFL4), median=median(IGFL4), sd=sd(IGFL4))
```


#### Step 3: Fit of oneway ANOVA, model validation

The scientific question is whether the gene expression level of IGFL4 differs between the three types/groups. Thus, the natural type of analysis is a oneway analysis of variance (ANOVA). The oneway ANOVA is fitted with `lm`. It is a good approach to assign a name (below *oneway*) to the object with the fitted model. This object contains all relevant information and may be used for subsequent analysis. Note that we have logarithmic transformed the response as intensities are often on a multiplicative scale.

```{r}
oneway <- lm(log(IGFL4) ~ type, data=psorData)
oneway
```


#### Step 4: Hypothesis test + Post hoc tests

It is standard to carry out an $F$-test for the overall effect of the explanatory (i.e. independent) variable. To be precise, the hypothesis is that the expected values are the same in _all_ groups. The most easy way to do this test is to use `drop1`. The option `test="F"` is needed to get the $F$-test:

```{r}
drop1(oneway,test="F")
```

Thus, the overall test for homogeneity between the groups show, that the groups are not all the same. But it might be that the gene expression in two of the three groups, say, are not significantly different. To investigate that we do post hoc testing. This is nicely done within the framework of *estimated marginal means* using the **emmeans** package. Here `emmeans` makes the estimated marginal means (that is the predicted gene expression IGFL4 on the log scale), and the `pairs` command provide post hoc pairwise comparisons (package automatically adjusts for multiple comparisons using the default tukey method):

```{r}
emmeans(oneway,~type)
pairs(emmeans(oneway,~type))
```


#### Step 5: Report of model parameters

In this example, it is natural to report the estimated intensities as well as the comparisons between the 3 groups. As the analysis was done using a log transformation it is also advisable to backtransform. Both things can be done automatically by the **emmeans** package, where the option `type="response"` (here `type` is the name of an option inside the R command, and it is a coincidence that it has the same name as the variable *type*) requests the backtransformation:

```{r}
emmeans(oneway,~type,type="response")
confint(pairs(emmeans(oneway,~type,type="response")))
pairs(emmeans(oneway,~type,type="response"))
```

Although we don't recommend usage of standard errors in the context of backtransformed parameters, we see that a standard error is provided on the backtransformed parameters.


## Example B: Simple and multiple linear regression

#### Step 1: Data

For the first part of the presentation we will use data from 31 cherry trees available in the built-in dataset `trees`: Girth, height and tree volume have been measured for each tree. Please note, that the help page `?trees` states, that girth is the tree diameter. Interest is in the association between tree volume on the one side, and girth and height on the other side. 

```{r}
data(trees)
trees
head(trees)
```

#### Step 2: Visualization of raw data

```{r, echo=TRUE, message=FALSE, fig.height=4, fig.width=4}
plot(trees)
```

A more fancy version of this is available in **GGally**, where the diagonal is used to visualize the marginal distribution of the three tree variables:
```{r, fig.height=4, fig.width=4}
ggpairs(trees)
```

#### Step 3: Fitting and validating a simple linear regression model

We start out with a simple linear regression with volume as outcome (i.e. dependent variable) and girth as covariate (i.e. independent variable). This model is fitted with the `lm` function. It is a good approach to assign a name (below *linreg1*) to the object with the fitted model. This object contains all relevant information and may be used for subsequent analysis.

```{r}
linreg1 <- lm(Volume ~ Girth, data=trees)
linreg1
```

Mathematically this model makes assumptions about linearity, variance homogeneity, and normality. These assumptions are checked visually via four plots generated from the model residuals. In R, it is easy to make these plots; simply apply `plot` on the `lm`-object.

```{r}
par(mfrow=c(2,2))   # makes room for 4=2x2 plots!
plot(linreg1)
```

In this case there appears to be problems with the model. In particular, the upper left plot shows a quadratic tendency suggesting that the linearity assumption is not appropriate. Datapoint 31 seems to be an outlier as well. There seems to be some potential skewness. Perhaps a data transformation can help us out. 

#### Step 4 iterated: Transformation

The linear regression model above says that an increment of $\Delta_g$ of girth corresponds to an increment of volume by $\beta\Delta_g$ where $\beta$ is the slope parameter (i.e. the coefficient), and hence, the interpretation is concerned with _absolute changes_. Perhaps it is more reasonable to talk about _relative changes_: An increment of girth by a factor $c_g$ corresponds to an increment of volume of $\gamma c_g$. This corresponds to a linear regression model on the log-transformed variables, which is easily fitted. Notice that `log` in R is the natural logarithm.

```{r}
linreg2 <- lm(log(Volume) ~ log(Girth), data=trees)
par(mfrow=c(2,2))
plot(linreg2)
```
This model looks better! Residuals vs. fitted values as well as no outliers in the leverage plot!


#### Step 5: Report of the model

Parameter estimates, standard errors are computed and certain hypothesis tests are carried out by the `summary` function:
```{r}
summary(linreg2)
```

The coefficient table is the most important part of the output. It has two lines --- one for each parameter in the model --- and four columns. The first line is about the intercept, the second line is about the slope. The columns are

* `Estimate`: The estimated value of the parameter (intercept or slope).
* `Std. Error`: The standard error (estimated standard deviation) associated with the estimate in the first column.
* `t value`: The $t$-test statistic for the hypothesis that the corresponding parameter is zero. Computed as the estimate divided by the standard error.
* `Pr(>|t|)`: The $p$-value associated with the hypothesis just mentioned. In particular the $p$-value in the second line is for the null hypothesis that there is no effect of girth on volume.


If you would like to report confidence intervals for the parameters (intercept and slope), then use `confint`:
```{r}
confint(linreg2)
```

Finally, don't forget to make the mathematical interpretation of the model and its parameter estimates! Thus, the model postulates the relation $$\ln V \approx \alpha + \beta \ln G$$ with estimates $\hat{\alpha} = -2.353$ and $\hat{\beta} = 2.200$. This corresponds to $V \approx e^\alpha \cdot G^\beta$, and we see that an increment of girth by 10%, say, corresponds to an increment of volume by a factor of $1.1^{2.2} = 1.23$, that is, by 23%.


#### Step 6: Visualization of the model

An excellent synthesis of a statistical analysis is often provided by a plot combining the raw data with the model fit.

The estimated regression line can be added to a scatter plot in different ways (and with/without corresponding standard error curves):

```{r, fig.height=3, fig.width=4}
ggplot(trees, aes(x = log(Girth), y = log(Volume))) + 
  geom_point() +
  geom_abline(intercept=-2.35332, slope=2.19997, col="red")

ggplot(trees, aes(x = Girth, y = Volume)) + 
  geom_point() +
  geom_smooth(method = "lm", se = TRUE) +
  scale_x_log10() +
  scale_y_log10()
```



#### Step 7 iterated: Multiple linear regression

Several covariates can be included in a regression model, corresponding to multiple linear regression; simply write a plus between the covariates. Here is model fit and validation with log-girth as well as log-height included in the model:

```{r}
linreg3 <- lm(log(Volume) ~ log(Girth) + log(Height), data=trees)
summary(linreg3)$coefficients
```

Don't forget to do model validation:
```{r}
par(mfrow=c(2,2))
plot(linreg3)
```


#### Step 8: Hypothesis test

The $F$-test for comparison of two nested models may be carried out by the `anova` function. For example, `linreg3` is nested in `linreg2`, and the comparison between the two corresponds to the null hypothesis that there is no extra information provided by height when girth is included (on log-scale) in the model.

```{r}
anova(linreg3, linreg2)
```

However, as an alternative to `anova` it is more pratical to use the `drop1` command, which makes all the tests corresponding to removal of one of the explanatory variables (i.e. independent variables). If you want $p$-values, then ask for $F$-tests:
```{r}
drop1(linreg3,test="F")
```

We see that `anova` and `drop1` give the same p-value ($p=7.8 * 10^{-6}$) for the effect of log(Height). And in a Gaussian model (like this) this coincides with the p-value from the $t$-test for the null hypothesis that there is no effect of a single numerical covariate i.e. height in our example. Thus, 

```{r}
summary(linreg3)$coefficients
```

So should you do all three tests? Definitely not! 

* In general, it is not recommendable to use `summary` for doing tests: The $F$-tests and $t$-tests only coincide for hypothesis on a single parameter. And you may also be deceived by the model parametrizations when interpreting $t$-tests.
* To use `drop1` you only need to fit the large model (here *linreg3*). Thus, it gives shorter and more easily readable R code.   

&nbsp;




## Outlook: Other analyses

The `lm` function is used for linear models, that is, models where data points are assumed to be independent with a Gaussian (i.e. normal) distribution (and typically also with the same variance). Obviously, these models are not always appropriate, and there exists functions for many, many more situations and data types. Here, we just mention a few functions corresponding to common data types and statistical problems. 

* `glm`: For independent, but non-Gaussian data. Examples are binary outcomes (logistic regression) and outcomes that are counts (Poisson regression). glm is short for generalized linear models, and the `glm` function is part of the base installation of R.
* `lmer` and `glmer`: For data with dependence structures that can be described by random effects, e.g., block designs. lme is short for linear mixed effects  (Gaussian data), glmer is short for generalized linear mixed effects  (binary or count data). Both functions are part of the **lme4** package. 
* `nls`: For non-linear regression, e.g., dose-response analysis. nls is short for non-linear least squares. The function is included in the base installation of R.

The functions mentioned above are used in a similar way as `lm`: a model is fitted with the function in question, and the model object subsequently examined with respect to model validation, estimation, computation of confidence limits, hypothesis tests, prediction, etc. with functions `summary`, `confint`, `drop1`, `emmeans`, `pairs` as indicated above. 
