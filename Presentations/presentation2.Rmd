---
title: "II. Working with data in R (presentation)"
author: "Data Science Lab, University of Copenhagen"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  #pdf_document: default
  html_document: default
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

## Tidyverse package 

The tidyverse is a collection of R packages which, among other things, facilitate data handling and data transformation in R. See <https://www.tidyverse.org/> for details. 

We must install and load the R package **tidyverse** before we have access to the functions. 

* Install package: One option is to go via the _Tools_ menu: *Tools* &#8594; *Install packages* &#8594; write `tidyverse` in the field called _Packages_. This only has to be done once. Otherwise use the `install.packages` function as shown here:  
```{r, message=FALSE}
install.packages("tidyverse", repos = "https://mirrors.dotsrc.org/cran/")
```

* Load package: Use the `library` command below (preferred), or go to the _Packages_ menu in the bottom right window, find **tidyverse** in the list, and click it. This has to be done in every R-session where you use the package. 
```{r, message=FALSE}
library(tidyverse)
```



People with SCIENCE PC's (Windows) sometimes have problems with the installation step because R tries to install files to a place, where the user doesn't have permissions to save and edit files. You can try this instead:

* When you start RStudio, right-click the icon and choose _Run as administrator_. Perhaps you can now install packages by clicking _Tools_ and _Include Packages_ as above. 

* If not, then the problem may be that RStudio is trying to install to your science drive (H: or \\a00143.science.domain). If so, try the command `.libPaths()`. If it shows two folders - one at the science drive and one locally one on your computer (`C:`) - then try the command `install.packages("tidyverse", lib=.libPaths()[2])`.


---

## About the working directory

When working on a project, it is important to know _where you are_. The working directory is the path on your computer that R will _try_ to access files from.

There are several helpful commands that help you navigate.

```{r, message=FALSE, eval=FALSE}
# show current working directory (cwd)
getwd()

# show folders in cwd
list.dirs(path = ".", recursive = FALSE)

# relative path, go one folder deeper into Presentations
setwd('./Presentations')

# go one step back in the directory
setwd('..')

# absolute path
setwd("~/Documents/work/Projects/FromExceltoR/Presentations")
```

---

## Import data

Data from Excel files can be imported via the _Import Dataset_ facility. You may get the message that the package **readxl** should be installed. If so, then install it as explained for **tidyverse** above. 

* Find _Import Data_ in the upper right window in RStudio, and choose _From Excel_ in the dropdown menu.

* A new window opens. Browse for the relevant Excel file; then a preview of the dataset is shown. Check that it looks OK, and click _Import_. 

* Three things happened: Three lines of code was generated (and executed) in the Console, a new dataset now appears in the Environment window, and the dataset is shown in the top left window. Check again that it looks OK. 

* Copy the first two lines of code into your R script (or into an R chunk in your Markdown document), but delete line starting with `View` and write instead the name of the dataset, here **downloads**. Then the first 10 lines of the data set are printed. 

```{r}
library(readxl)
downloads <- read_excel("downloads.xlsx")
downloads
```

R has stored the data in a so-called _tibble_, a type of data frame. 
Rows are referred to as *observations* or *data lines*, columns as *variables*.
The data rows appear in the order as in the Excel file.

A slight digression: If data are saved in a csv file (comma separated values), possibly generated via an Excel sheet, 
then data can be read with the read_csv function. For example, if the data file is called `mydata.csv` and values are separated with commas, then the command 
```{r, echo=TRUE, eval=FALSE}
mydata <- read.csv("mydata.csv", sep=",")
```
creates a data frame in R with the data. The data frame is _not_ a tibble and some of the commands below would not work for such a data frame. 


---

## About the data

The dataset is from Boston University and is about www data transfers from November 1994 to May 1995, see <http://ita.ee.lbl.gov/html/contrib/BU-Web-Client.html>.


* It has 147,035 data lines and 6 variables

* _size_ is the download size in bytes, and _time_ is the download time in seconds.

---

## Extracting variables, simple summary statistics

Variables can be extracted with the $-syntax, and we can use squared brackets to show only the first 40, say, values. 

```{r}
time_vector <- downloads$time
time_vector[1:40]
```

Summary statistics like mean, standard deviation, median are easily computed for a vector. 

Examples of R functions for computing summary statistics: `length`, `mean`, `median`, `sd`, `var`, `sum`, `quantile`, `min`, `max`, `IQR`.



```{r}
length(time_vector)
mean(time_vector)
sd(time_vector)
median(time_vector)
min(time_vector)
```

Notice that more than half the observations have time equal to zero (median is zero).

---

## Filtering data (selecting rows): `filter`

The `filter` function is used to make sub-datasets where only certain datalines (rows) are maintained. It's described with _logical expressions_ which datalines should be kept in the dataset.

Say that we only want observations with download time larger than 1000 seconds; there happens to be eight such observations: 

```{r}
filter(downloads, time > 1000)
```

Or say that only want observations with strictly positive download size:

```{r}
downloads2 <- filter(downloads, size > 0)
downloads2
```

Notice that this result is assigned to **downloads2**. It has 36,708 data lines. The original data called **downloads** still exists with 147,035 data lines.

Filtering requires *logical predicates*. These are expressions in terms of columns, which evaluate to either `TRUE` or `FALSE` for each row. Logical expressions can be combined with logical operations.

* Comparisons: ` == `, ` != `, ` < `, ` > `, ` <= `, ` >= `, ` %in% `, ` is.na `

* Logical operations: `!` (not), `|` (or), `&` (and). A comma can be used instead of `&`

Here comes two sub-datasets:


```{r}
# Rows from kermit, and with size greater than 200000 bytes are kept.
filter(downloads2, machineName == "kermit", size > 200000)

# Rows NOT from kermit, and with size greater than 200000 bytes are kept.
filter(downloads2, machineName != "kermit" & size > 200000)
```

A helpful function to know which machine names are valid can be:

```{r}
# get unique machineName values in downloads2
distinct(downloads2, machineName)
```

And if you are looking for multiple values for a given variable:

```{r}
downloads2 %>% filter(machineName %in% c("kermit","pluto"), size > 2000000)
```

---

## Selecting variables: `select`

Sometimes, datasets has many variables of which only some are relevant for the analysis. Variables can be selected or skipped with the `select` function.

```{r}
# Without the date variable
select(downloads2, -date)

# Only include the three mentioned variable names
downloads3 <- select(downloads2, machineName, size, time)
downloads3
```

Notice that we have made a new dataframe, **downloads3** with only three variables. 


---

## Transformations of data

Tranformations of existing variables in the data set can be computed and included in the data set with the `mutate` function. 

We first compute two new variables, download speed (**speed**) and the logarithm of the download size (**logSize**):

```{r}
downloads3 <- mutate(downloads3, speed = size / time, logSize = log10(size))
downloads3
```

We then make a new categorial variable, **slow**, which is "Yes" is speed < 150 and "No" otherwise

```{r}
downloads3 <- mutate(downloads3, slow = ifelse(speed < 150, "Yes", "No"))
downloads3
```

---

## Counting, tabulation of categorical variables: `count`

The `count` function is useful for counting data datalines, possibly according to certain criteria or for the different levels of categorical values.



```{r}
# Total number of observations in the current dataset
count(downloads3)

# Number of observations from each machine
count(downloads3, machineName)

# Number of observations which have/have not size larger than 5000
count(downloads3, size>5000)

# Number of observations for each combination of machine name and the *slow* variable.
count(downloads3, machineName, slow)
```

---

## Sorting data: `arrange`

The `arrange` function can be used to sort the data according to one or more columns.

Let's sort the data according to download size (ascending order). The first lines of the sorted data set is printed on-screen, but the dataset **downloads3** has *not* been changed.

```{r}
arrange(downloads3, size)
```

Two different examples:

```{r}
# According to download size in descending order
arrange(downloads3, desc(size))

# After machine name and then according to download size in descending order
arrange(downloads3, machineName, desc(size))
```

---

## Grouping: `group_by`

We can group the dataset by one or more categorical variables with `group_by`. The dataset is not changed as such, but - as we will see - grouping can be useful for computation of summary statistics and graphics. 

Here we group after machine name (first) _and_ the slow variable (second). The only way we can see it at this point is in the second line in the output (`# Groups:`):

```{r}
# Group according to machine
group_by(downloads3, machineName)

# Group according to machine and slow
group_by(downloads3, machineName, slow)
```

---

## Summary statistics, revisited: `summarize`

Recall how we could compute summary statistics for a single variable in a dataset, e.g.

```{r}
mean(downloads3$size)
max(downloads3$size)
```

With `summarize` we can compute summary statistics for a variable for each level of a grouping variable or for each combination of several grouping variables. 

First, a bunch of summaries for the size variable for each machine name, where we give explicit names for the new variables:

```{r}
downloads.grp1 <- group_by(downloads3, machineName)
summarize(downloads.grp1, 
          avg = mean(size),
          med = median(size),
          stdev = sd(size),
          total = sum(size),
          n = n())
```

Second, the same thing but for each combination of machine name and the slow variable:

```{r}
downloads.grp2 <- group_by(downloads3, machineName, slow)
summarize(downloads.grp2, 
          avg = mean(size),
          med = median(size),
          stdev = sd(size),
          total = sum(size),
          n = n())
```


Third, mean and standard deviation for several variables: 


```{r}
summarize_at(downloads.grp2, c("time", "size"), list(ave=mean,stdev=sd))
```

The datasets with summaries can be saves as datasets themselves, for example to be used as the basis for certain graphs. 

---

## The pipe operator: `%>%`

Two or more function calls can be evaluated sequentially using the so-called pipe operator, `%>%`. Nesting of function calls becomes more readable, and intermediate assignments are avoided.

Let's try it to do a bunch of things in one go, starting with the original dataset:

```{r}
downloads %>% 
  filter(size>0) %>% # Subset of data
  group_by(machineName) %>% # Grouping 
  summarize(avg = mean(size)) %>% # Compute mean
  arrange(avg) # Sort after mean
```

